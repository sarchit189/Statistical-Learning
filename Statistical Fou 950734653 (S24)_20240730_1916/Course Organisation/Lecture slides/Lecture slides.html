<!doctype html>
<html>
<head>
    <title>Lecture slides</title>
    <meta charset="utf-8">
    
</head>
<body>
<p></p><ul><li>Compiled lecture slides&nbsp;(for exam) - to be added</li></ul><p></p><strong>Main content</strong><br><ul><li><a href="data/0-organisation.pdf">Organisation</a></li></ul><ul><li><strong>Part I - Binary classification</strong>
  </li>
</ul>
<ol>
  <li><a href="data/1-stat-learning.pdf">Statistical learning problem</a> (includes risk minimisation, ERM, Bayes risk)&nbsp;</li>
  <li><a href="data/2-kNN.pdf">k-Nearest neighbour classification</a> (includes universal consistency)</li>
  <li><a href="data/3-ERMfiniteH.pdf">ERM over finite hypothesis classes</a></li>
  <li><a href="data/4-ERMinfiniteH.pdf">Uniform convergence</a> (includes generalisation error bound for infinite hypothesis classes)</li>
  <li><a href="data/5-VCdimension.pdf">VC dimension and Sauer's lemma</a>&nbsp;(includes error bounds for linear classifiers, neural networks)</li>
  <li><a href="data/6-PAClearning.pdf">PAC learning</a> (includes&nbsp;<span style="font-size: calc(0.90375rem + 0.045vw);">No Free Lunch theorem and Fundamental theorem)</span>
  </li>
  <li><a href="data/7-Stability-Validation.pdf">Algorithmic stability&nbsp;and validation</a></li>
  <li><span style="font-size: 0.9375rem;"><a href="data/8-SVM.pdf">Support vector machines</a></span>
  </li>
  <li><a href="data/9-convex.pdf">Convex learning</a>&nbsp;(includes surrogate loss minimisation and Tikhonov regularisation)</li>
  <li><span style="font-size: 0.9375rem;"><a href="data/10-ensemble.pdf">Ensemble methods</a> (includes bagging, boosting)</span></li>
</ol><p></p><ul><li><strong>Part II - Regression and Clustering</strong></li></ul>&nbsp;&nbsp; 11. &nbsp;<a href="data/11-regression.pdf">Regression: Universal approximation and generalisation bounds</a><br>&nbsp;&nbsp; 12. &nbsp;<a href="data/12-kmeans.pdf">k-means clustering</a> (includes approximation guarantees for k-means++ and explainable k-means)<br>&nbsp; &nbsp;13. &nbsp;<a href="data/13-hierarchical.pdf">Hierarchical clustering</a>
<br><span style="font-size: calc(0.90375rem + 0.045vw);"><br></span><br><ul><li><strong>Additional slides (not part of exam):</strong>&nbsp;</li></ul>&nbsp;&nbsp;14. &nbsp;<a href="data/14-nngp-ntk.pdf">Neural tangent kernel and Neural Network Gaussian process</a><br>&nbsp; 15. &nbsp;Exact risk of linear regression and double descent phenomenon<br><br>
<ul>
  <li><strong>Appendix:</strong>&nbsp;</li>
</ul>&nbsp; &nbsp; <span style="font-size: 0.9375rem; background-color: rgb(255, 255, 255);"><a href="data/Probability%20Inequalities%20%281%29.pdf">Probability Inequalities</a></span>
<br>&nbsp; &nbsp; <a href="data/terminology.pdf?time=1652776612708">Some notations (could be incomplete)</a>
<br>&nbsp; &nbsp; &nbsp;&nbsp;<br><ul><li><strong>Main references</strong><strong>:</strong>
  </li>
</ul><span><ol><li>Shai Shalev-Shwartz, and Shai Ben-David.&nbsp;<em>Understanding machine learning: From theory to algorithms</em>. Cambridge university press, 2014. [<a href="https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf">link</a>]<br></li><li>Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar.&nbsp;<em>Foundations of Machine Learning.</em>MIT Press, 2nd Ed., 2018. [<a href="https://cs.nyu.edu/~mohri/mlbook/">link</a>]<br></li><li>Luc Devroye, László Györfi, and Gábor Lugosi.&nbsp;<em>A probabilistic theory of pattern recognition</em>. Vol. 31. Springer Science &amp; Business Media, 2013. [<a href="https://www.szit.bme.hu/~gyorfi/pbook.pdf">link</a>]</li><li><span style="font-size: 0.9375rem;">Matus Telgarsky.&nbsp;</span><em style="font-size: 0.9375rem;">Deep learning theory lecture notes</em><span style="font-size: 0.9375rem;">, UIUC, 2020. [</span><a href="https://mjt.cs.illinois.edu/dlt/index.pdf" style="font-size: 0.9375rem;">link</a><span style="font-size: 0.9375rem;">]</span><br></li><li><span style="font-size: 0.9375rem;">Sanjoy Dasgupta.&nbsp;</span><em style="font-size: 0.9375rem;">Lecture notes: Topics in unsupervised learning</em><span style="font-size: 0.9375rem;">, UCSD, 2008. [<a href="https://cseweb.ucsd.edu/~dasgupta/291-unsup/">link</a>]</span><br></li><li><span style="font-size: 0.9375rem;">Sanjoy Dasgupta. "Algorithms for k-means clustering". </span><em style="font-size: 0.9375rem;">Lecture notes: Geometric algorithms</em><span style="font-size: 0.9375rem;">, UCSD, 2013. [</span><a href="https://cseweb.ucsd.edu/~dasgupta/291-geom/kmeans.pdf" style="font-size: 0.9375rem;">link</a><span style="font-size: 0.9375rem;">]</span></li><li><span style="font-size: 0.9375rem;">Gérard Biau, Luc Devroye, and Gäbor Lugosi. "Consistency of random forests and other averaging classifiers."&nbsp;</span><em style="font-size: 0.9375rem;">Journal of Machine Learning Research</em><span style="font-size: 0.9375rem;">&nbsp;9.9, 2008. [</span><a href="https://www.jmlr.org/papers/volume9/biau08a/biau08a.pdf" style="font-size: 0.9375rem;">link</a><span style="font-size: 0.9375rem;">]</span></li><li>Sanjoy Dasgupta. "A cost function for similarity-based hierarchical clustering",&nbsp;<em>Proceedings of the 48th Annual ACM SIGACT Symposium on Theory of Computing (STOC)</em>, pp.&nbsp;118–127, 2016. [<a href="https://dl.acm.org/doi/pdf/10.1145/2897518.2897527">link</a>]<br></li><li>Vincent Cohen-Addad, Varun Kanade, Frederik Mallmann-Trenn, and Claire Mathieu. "Hierarchical clustering: Objective functions and algorithms". <em>Journal of the ACM</em>, 66(4):1-42, 2019. [<a href="https://dl.acm.org/doi/pdf/10.1145/3321386">link</a>]<br></li><li><span style="font-size: 0.9375rem;">Ludwig Bothmann </span><em style="font-size: 0.9375rem;">et al</em><span style="font-size: 0.9375rem;">. "</span><span style="font-size: 0.9375rem;">Chapter 18.04: Reproducing Kernel Hilbert Space and Representer Theorem", <em>Lecture notes: Introduction to Machine Learning</em>, LMU. [<a href="https://slds-lmu.github.io/i2ml/chapters/18_nonlinear_svm/18-04-rkhs-repr/">link</a>]</span><br></li><li>Adityanarayanan Radhakrishnan. <em>Lecture notes: Modern Machine Learning </em>[<a href="https://web.mit.edu/modernml/course/index.html">link</a>]</li></ol></span>&nbsp; &nbsp; &nbsp;&nbsp;<br>
<ul>
  <li><strong>Additional references</strong><strong>:</strong> Some results from the references are mentioned in lecture but not discussed in detail
  </li>
</ul><span><ol><li><span style="font-size: 0.9375rem;">Olivier Bousquet, and André Elisseeff. "Stability and generalization."&nbsp;</span><em style="font-size: 0.9375rem;">The Journal of Machine Learning Research</em><span style="font-size: 0.9375rem;">&nbsp;2 (2002): 499-526.</span><br></li><li>Amit Daniely, et al.&nbsp; "Multiclass learnability and the erm principle."&nbsp;<em>Proceedings of the 24th Annual Conference on Learning Theory</em>. JMLR Workshop and Conference Proceedings, 2011.<br></li><li>Ilias Diakonikolas, et al. "Boosting in the presence of massart noise."&nbsp;<em>Conference on Learning Theory</em>. PMLR, 2021.<br></li><li>Kasper G. Larsen. "Bagging is an Optimal PAC Learner."&nbsp;<em>Conference on Learning Theory</em>. PMLR, 2023.<br></li><li>Shai Ben-David, Philip M. Long, and Yishay Mansour. "Agnostic boosting."&nbsp;<em>14th Annual Conference on Computational Learning Theory, COLT 2001</em>.<br></li><li><span style="font-size: 0.9375rem;">David Arthur and Sergei Vassilvitskii. "k-means++: The advantages of careful seeding."&nbsp;</span><span style="font-size: 0.9375rem;">In <em>ACM-SIAM SODA</em>, pages 1027–1035, 2007.</span></li><li><span style="font-size: 0.9375rem;">Sara Ahmadian, et al. "Better Guarantees for k-Means and Euclidean k-Median by Primal-Dual Algorithms."&nbsp;</span><span style="font-size: 0.9375rem;">In <em>FOCS</em>, 2017.</span></li><li><span style="font-size: 0.9375rem;">E. Lee, M. Schmidt, J. Wright. "Improved and simplified inapproximability for k-means." <em>Information Process Letters</em>, 2017.</span><br></li><li><span style="font-size: 0.9375rem;">David Pollard. "Strong consistency of k-means clustering." <em>The Annals of Statistics</em>, 1981.</span><br></li><li>David Pollard. "A central limit theorem for k-means clustering." <em>The Annals of Probability</em>, 1982.</li><li><span style="font-size: 0.9375rem;">Boris Hanin, and Mark Sellke. "Approximating continuous functions by ReLU nets of minimum width", arXiv:1710.11278</span></li><li><span style="font-size: 0.9375rem;">Michael M. Wolf.&nbsp;</span><em style="font-size: 0.9375rem;">Lecture notes: Mathematical foundations of supervised learning</em><span style="font-size: 0.9375rem;">, TUM. [<a href="https://mediatum.ub.tum.de/doc/1723378/1723378.pdf">link</a>]</span></li><li><span style="font-size: 0.9375rem;"><br><br></span></li></ol></span>&nbsp;
&nbsp; &nbsp;
</body>
</html>